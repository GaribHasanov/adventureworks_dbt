# ğŸš€ AdventureWorks Data Warehouse with dbt

## ğŸ“Œ Project Overview

This project implements an end-to-end modern Data Warehouse using the AdventureWorks dataset.

The pipeline follows a structured ELT approach:

- CSV files are ingested using Python
- Raw tables are created in PostgreSQL (`raw` schema)
- dbt transforms raw data into staging models
- Final fact and dimension tables are built in the mart layer

The project demonstrates production-style data engineering practices including layered modeling, schema separation, data testing, and modular SQL transformations.

---

## ğŸ—ï¸ Architecture

This project follows a modern ELT architecture:

1. **Extract**
   - Source data stored as CSV files.

2. **Load**
   - Python script loads CSV files into PostgreSQL.
   - Tables are created inside the `raw` schema.

3. **Transform**
   - dbt reads raw tables.
   - Staging models clean and standardize data.
   - Mart models build analytics-ready fact and dimension tables.

Data Flow:

CSV Files  
â†“  
PostgreSQL (`raw` schema)  
â†“  
dbt Staging Models (`adventureworks` schema)  
â†“  
dbt Mart Models (`adventureworks` schema)  

---

## ğŸ§° Tech Stack

- Python (CSV ingestion)
- PostgreSQL (Data Warehouse)
- dbt (Data transformations)
- SQL
- Git & GitHub

---

## âš™ï¸ Environment Configuration

The Python scripts use a `.env` file to load PostgreSQL connection details (host, port, username, password, database).  

**Important:** The `.env` file is **gitignored** for security reasons and is **not included in the repository**.  

To run the scripts, create a `.env` file in the same folder as the scripts with the following variables:

```env
POSTGRES_HOST=<your_postgres_host>
POSTGRES_PORT=<your_postgres_port>
POSTGRES_USER=<your_username>
POSTGRES_PASSWORD=<your_password>
POSTGRES_DB=<your_database_name>
```

Replace the placeholders with your actual PostgreSQL credentials.

---

## ğŸ“‚ Project Structure

```
project/
â”œâ”€â”€ python/
â”‚   â””â”€â”€ load_csv_to_postgres.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ adventureworks_customer_lookup.csv
â”‚   â”œâ”€â”€ adventureworks_product_categories_lookup.csv
â”‚   â”œâ”€â”€ adventureworks_product_lookup.csv
â”‚   â”œâ”€â”€ adventureworks_product_subcategories_lookup.csv
â”‚   â”œâ”€â”€ adventureworks_returns_data.csv
â”‚   â”œâ”€â”€ adventureworks_sales_data_2022.csv
â”‚   â””â”€â”€ adventureworks_territory_lookup.csv
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ stg_customers.sql
â”‚   â”‚   â”œâ”€â”€ stg_product_categories.sql
â”‚   â”‚   â”œâ”€â”€ stg_product_subcategories.sql
â”‚   â”‚   â”œâ”€â”€ stg_product.sql
â”‚   â”‚   â”œâ”€â”€ stg_returns.sql
â”‚   â”‚   â”œâ”€â”€ stg_sales.sql
â”‚   â”‚   â”œâ”€â”€ stg_territory.sql
â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”‚
â”‚   â”œâ”€â”€ mart/
â”‚   â”‚   â”œâ”€â”€ dim_customers.sql
â”‚   â”‚   â”œâ”€â”€ fct_returns.sql
â”‚   â”‚   â”œâ”€â”€ fct_sales.sql
â”‚   â”‚   â””â”€â”€ schema.yml
â”‚
â””â”€â”€ dbt_project.yml
```

---

## ğŸ—„ï¸ Raw Layer (PostgreSQL)

Python loads CSV data into the `raw` schema:

- raw.adventureworks_customer_lookup  
- raw.adventureworks_product_categories_lookup  
- raw.adventureworks_product_lookup  
- raw.adventureworks_product_subcategories_lookup  
- raw.adventureworks_returns_data  
- raw.adventureworks_sales_data_2022  
- raw.adventureworks_territory_lookup  

This layer represents the source-aligned raw data.

---

## ğŸ§± Staging Layer (dbt)

The staging layer:

- Cleans column names
- Applies data type standardization
- Removes inconsistencies
- Prepares data for analytics

Staging models:

- stg_customers
- stg_product_categories
- stg_product_subcategories
- stg_product
- stg_returns
- stg_sales
- stg_territory

---

## â­ Mart Layer (Analytics Layer)

The mart layer builds a Star Schema for analytical queries.

### Dimension Tables

- dim_customers

### Fact Tables

- fct_sales
- fct_returns

This layer is optimized for BI and reporting.

---

## ğŸ§ª Data Testing

dbt tests are defined inside `schema.yml` files.

Included tests:

- Not Null tests
- Unique tests
- Relationship tests

Run tests:

```bash
dbt test
```

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Load raw data

Run the Python script to load CSV files into PostgreSQL:

```bash
python load_to_adventureworks_calendar_lookup.py
python load_to_adventureworks_customer_lookup.py
python load_to_adventureworks_product_categories_lookup.py
python load_to_adventureworks_product_lookup.py
python load_to_adventureworks_product_subcategories_lookup.py
python load_to_adventureworks_returns_data.py
python load_to_adventureworks_sales_data_2022.py
python load_to_adventureworks_territory_lookup.py
```

### 2ï¸âƒ£ Run dbt models

```bash
dbt run
```

### 3ï¸âƒ£ Run tests

```bash
dbt test
```

### 4ï¸âƒ£ Generate documentation

```bash
dbt docs generate
dbt docs serve
```

---

## ğŸ¯ Key Data Engineering Concepts Demonstrated

- End-to-end ELT pipeline
- Schema separation (raw vs analytics)
- Layered data modeling (staging â†’ mart)
- Star schema design
- Data validation with dbt tests
- Modular SQL transformations
- Version-controlled data project

---

## ğŸš€ Future Improvements

- Add dbt snapshots
- Add CI/CD pipeline (GitHub Actions)
- Add data quality monitoring
- Add BI dashboard integration

---

## ğŸ‘¤ Author

Garib Hasanov  
Data & Analytics Engineer

---

## ğŸ“„ License

This project is for educational and portfolio purposes.
